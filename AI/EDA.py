# 1. è®€å…¥è³‡æ–™èˆ‡åŸºæœ¬è¨­å®š
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sqlalchemy import create_engine

from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline  # ç”¨æ–¼ SMOTE ç­‰è™•ç†
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, roc_auc_score, roc_curve,
    precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import shap
import warnings
warnings.filterwarnings("ignore")

# è®€å– MySQL ä¸­çš„ kaggle è³‡æ–™è¡¨
engine = create_engine("mysql+pymysql://root:6363@localhost:3306/stroke")
kaggle = pd.read_sql("SELECT * FROM strokedata;", engine)

# è™•ç† BMI æ¬„ä½ï¼ˆè½‰æ•¸å­— + è£œä¸­ä½æ•¸ï¼‰
kaggle["bmi"] = pd.to_numeric(kaggle["bmi"], errors="coerce")
kaggle["bmi"] = kaggle["bmi"].fillna(kaggle["bmi"].median())

# é¡¯ç¤ºç¼ºå¤±å€¼å ±å‘Š
def missing_report(df):
    miss = df.isnull().sum()
    return pd.DataFrame({
        "missing_count": miss,
        "missing_pct": miss / len(df) * 100
    }).sort_values("missing_pct", ascending=False)

print("Kaggle missing:\n", missing_report(kaggle))

# åˆ†é¡æ¬„ä½è£œå……è™•ç†
categorical_cols = ['gender','ever_married','work_type','Residence_type','smoking_status']
categorical_cols = [c for c in categorical_cols if c in kaggle.columns]

# ç‰¹å¾µå·¥ç¨‹ï¼šæ–°å¢ age group
def add_age_groups(df):
    df['age_group'] = pd.cut(df['age'], bins=[0,40,60,80,120], labels=['<40','40-59','60-79','80+'])
    return df

kaggle = add_age_groups(kaggle)

# æ•¸å€¼èˆ‡é¡åˆ¥æ¬„ä½å®šç¾©
num_features = ['age','bmi','avg_glucose_level']
num_features = [c for c in num_features if c in kaggle.columns]
cat_features = [c for c in categorical_cols if c in kaggle.columns]

# é è™•ç† pipeline
num_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])

# åˆ†å‰²è³‡æ–™
TARGET = 'stroke'
X = kaggle.drop(columns=[TARGET])
y = kaggle[TARGET]
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# å»ºç«‹æ¨¡å‹
models = {
    'lr': LogisticRegression(max_iter=1000, class_weight='balanced'),
    'rf': RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced'),
    'xgb': XGBClassifier(n_estimators=200, scale_pos_weight=10, eval_metric='logloss', random_state=42)
}

def evaluate_model(pipe, X_tr, y_tr, X_te, y_te):
    pipe.fit(X_tr, y_tr)
    y_pred = pipe.predict(X_te)
    y_prob = pipe.predict_proba(X_te)[:, 1]
    print(classification_report(y_te, y_pred, digits=4))
    auc = roc_auc_score(y_te, y_prob)
    print("ROC AUC:", auc)
    return pipe, auc

# è¨“ç·´æ¨¡å‹ä¸¦æ¯”è¼ƒ
best_auc = 0
best_model = None
best_name = ""

for name, clf in models.items():
    print("="*40)
    print(f"Training {name}")
    pipe = ImbPipeline([
        ('preproc', preprocessor),
        ('clf', clf)
    ])
    trained_pipe, auc = evaluate_model(pipe, X_train, y_train, X_val, y_val)
    if auc > best_auc:
        best_auc = auc
        best_model = trained_pipe
        best_name = name

print(f"\nâœ… Best model: {best_name.upper()} (AUC={best_auc:.4f})")

# è©•ä¼°åœ–å½¢
y_prob = best_model.predict_proba(X_val)[:, 1]
fpr, tpr, _ = roc_curve(y_val, y_prob)
plt.plot(fpr, tpr, label=f"{best_name.upper()} (AUC={best_auc:.3f})")
plt.plot([0,1],[0,1],'k--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# PR æ›²ç·š
prec, rec, _ = precision_recall_curve(y_val, y_prob)
plt.plot(rec, prec, label=best_name)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# æ··æ·†çŸ©é™£
y_pred = (y_prob >= 0.5).astype(int)
cm = confusion_matrix(y_val, y_pred)
ConfusionMatrixDisplay(cm).plot()
plt.title("Confusion Matrix")
plt.show()

# === SHAP åˆ†æ ===
clf = best_model.named_steps["clf"]
X_train_pre = best_model.named_steps["preproc"].transform(X_train)
X_val_pre = best_model.named_steps["preproc"].transform(X_val)

# ç‰¹å¾µåç¨±æå–å‡½æ•¸
def get_feature_names_from_column_transformer(column_transformer):
    output_features = []
    for name, transformer, cols in column_transformer.transformers_:
        if transformer == "drop":
            continue
        if transformer == "passthrough":
            output_features.extend(cols)
        else:
            if hasattr(transformer, "get_feature_names_out"):
                output_features.extend(transformer.get_feature_names_out(cols))
            else:
                output_features.extend(cols)
    return output_features

feat_names = get_feature_names_from_column_transformer(best_model.named_steps['preproc'])

# å»ºç«‹ explainer ä¸¦è¨ˆç®— shap å€¼
if isinstance(clf, LogisticRegression):
    masker = shap.maskers.Independent(X_train_pre)
    explainer = shap.LinearExplainer(clf, masker=masker)
    shap_values = explainer(X_val_pre)
    shap_vals = shap_values.values
elif isinstance(clf, (RandomForestClassifier, XGBClassifier)):
    explainer = shap.TreeExplainer(clf)
    shap_values = explainer.shap_values(X_val_pre)
    shap_vals = shap_values[1] if isinstance(shap_values, list) else shap_values
else:
    raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {type(clf)}")

# ç‰¹å¾µé‡è¦æ€§è¨ˆç®—èˆ‡é¡¯ç¤º
shap_importance = np.abs(shap_vals).mean(axis=0)
shap_df = pd.DataFrame({
    "Feature": feat_names,
    "Importance": shap_importance
})
shap_df["Percentage"] = shap_df["Importance"] / shap_df["Importance"].sum() * 100
shap_df = shap_df.sort_values("Importance", ascending=False).reset_index(drop=True)

print("\nğŸ“Š SHAP ç‰¹å¾µé‡è¦æ€§ï¼ˆå‰10åï¼‰:")
print(shap_df.head(10).round(2))

# SHAP æ¢å½¢åœ–
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=shap_df, palette="viridis")
plt.title(f"Feature Importance based on SHAP ({best_name.upper()})")
plt.xlabel("Mean |SHAP value|")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# ç¹ªåœ–
feat_names = get_feature_names_from_column_transformer(best_model.named_steps['preproc'])
shap.summary_plot(shap_values, X_val_pre, feature_names=feat_names)
# å„²å­˜è‡³ Excel
shap_df.to_excel("feature_importance_shap.xlsx", index=False)
print("âœ… SHAPç‰¹å¾µé‡è¦æ€§è¡¨æ ¼å·²å„²å­˜ç‚º 'feature_importance_shap.xlsx'")
