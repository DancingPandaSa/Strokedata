# ğŸ§  Stroke Prediction Data Pipeline

A full-stack data pipeline project for analyzing and predicting stroke risks using real-world healthcare data.

---

## ğŸ“Š Project Overview

This project demonstrates an **end-to-end data pipeline** that includes data ingestion, transformation, modeling, quality checks, storage, and visualization using modern data tools and best practices.

- **ğŸ“¦ Data Source**: [Kaggle - Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)
- **ğŸ¯ Goal**: Clean and transform healthcare data to analyze and predict stroke risks
- **ğŸ§± Architecture**: Modular pipeline using Python, MySQL, DBT, and visualization tools

---

## ğŸ§­ System Architecture

```text
è³‡æ–™ä¾†æº (Kaggle Dataset)
        â†“
ETL (Python / Pandas / SQL)
        â†“
è³‡æ–™å€‰å„² (MySQL)
        â†“
æ•¸æ“šå»ºæ¨¡ (DBT: staging / intermediate / mart)
        â†“
è³‡æ–™å“è³ªç›£æ§ (Python rule checks + logs)
        â†“
å¯è¦–åŒ– (Power BI)
        â†“
ç‰ˆæœ¬æ§ç®¡ + è‡ªå‹•åŒ– (Git + GitHub Actions)

```

---

## ğŸ›  Tech Stack

| Stage                   | Tool / Language           |
|-------------------------|---------------------------|
| Data Ingestion          | Python, Pandas            |
| SQL Query Engine        | DuckDB (local analysis)   |
| Data Warehouse          | MySQL                     |
| Data Modeling           | DBT                       |
| Data Quality Monitoring | Python (custom rules)     |
| Visualization           | Streamlit / Power BI      |
| CI/CD & Version Control | Git + GitHub Actions      |

---

## ğŸ“ Project Structure

```text
stroke-prediction-pipeline/
â”‚
â”œâ”€â”€ data/                            # Raw and cleaned datasets
â”‚   â””â”€â”€ healthcare-dataset-stroke-data.csv
â”‚
â”œâ”€â”€ etl/                             # ETL pipeline scripts
â”‚   â””â”€â”€ load_data.py                 # Cleans and loads data into MySQL
â”‚
â”œâ”€â”€ dbt/                             # DBT project folder
â”‚   â””â”€â”€ stroke_project/              # Contains DBT models (staging, mart, etc.)
â”‚
â”œâ”€â”€ quality_checks/                 # Data validation scripts
â”‚   â””â”€â”€ validate_data.py            # Rule-based quality checks and logging
â”‚
â”œâ”€â”€ dashboards/                      # Visualization layer
â”‚   â”œâ”€â”€ streamlit_app.py            # Streamlit dashboard app (optional)
â”‚   â””â”€â”€ powerbi_dashboard.pbix      # Power BI dashboard file (optional)
â”‚
â”œâ”€â”€ .github/                         # GitHub Actions CI/CD config
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ etl_ci.yml              # Workflow for automated ETL validation
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation
```
## âš™ï¸ Setup Instructions

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/stroke-prediction-pipeline.git
cd stroke-prediction-pipeline
```
### 2. Install Python Dependencies

Install all required Python libraries using the `requirements.txt` file:

```bash
pip install -r requirements.txt

```
### 3. Load Data into MySQL

Update MySQL credentials in etl/load_data.py, then run:
```bash
python etl/load_data.py
```
### 4. Run DBT Models

Navigate to the DBT project directory and execute:
```bash
cd dbt/stroke_project
dbt run
dbt test
```
### 5. Start Streamlit Dashboard (Optional)
streamlit run dashboards/streamlit_app.py

## âœ… Key Features

- ğŸ§¼ **Data Cleaning** with Pandas  
  Handle missing values, outlier filtering, gender normalization, and feature encoding (e.g., BMI median fill, gender uppercase trimming).

- ğŸ— **DBT Data Models**
  - `stg_stroke_data` â€“ Raw data staging layer
  - `int_patient_risk` â€“ Transformed intermediate layer
  - `mart_stroke_summary` â€“ Final reporting-ready dataset

- ğŸ§ª **Data Validation**  
  Custom Python checks for:
  - Null values
  - Data types
  - Business rules (e.g., valid age range, bmi > 0, etc.)

- ğŸ“Š **Interactive Dashboards**  
  Visualize key trends and KPIs using:
  - Power BI (optional)

- ğŸ” **CI/CD with GitHub Actions**  
  Automate data quality checks and ETL validation on every push or pull request.

- ğŸ”’ **Version Control**  
  Project managed with Git, following modular, reusable, and production-ready folder structure.

