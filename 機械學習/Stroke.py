import pandas as pd
from sqlalchemy import create_engine
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# 1. å¾ MySQL è®€å–è³‡æ–™
engine = create_engine("mysql+pymysql://root:6363@localhost:3306/stroke")
df = pd.read_sql("SELECT * FROM stg_strokedata;", engine)

# 2. ç›®æ¨™è®Šæ•¸ & ç‰¹å¾µ
y = df['stroke']
X = df.drop(columns=['stroke', 'id'])

# 3. é¡åˆ¥å‹ç‰¹å¾µ â†’ One-Hot Encoding
X = pd.get_dummies(X, drop_first=True)
feature_names = X.columns

# 4. æ‹†åˆ†è³‡æ–™é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# --- æ¨¡å‹ 1: Logistic Regression ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr = LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42)
lr.fit(X_train_scaled, y_train)
y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]
auc_lr = roc_auc_score(y_test, y_prob_lr)

# --- æ¨¡å‹ 2: Random Forest ---
rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced")
rf.fit(X_train, y_train)
y_prob_rf = rf.predict_proba(X_test)[:, 1]
auc_rf = roc_auc_score(y_test, y_prob_rf)

# --- æ¯”è¼ƒçµæœ ---
print(f"Logistic Regression AUC: {auc_lr:.4f}")
print(f"Random Forest AUC: {auc_rf:.4f}")

# --- é¸å‡ºæœ€ä½³æ¨¡å‹ ---
if auc_lr > auc_rf:
    print("âœ… Logistic Regression è¡¨ç¾æ¯”è¼ƒå¥½ï¼Œå»ºè­°ä½¿ç”¨å®ƒ")
    best_model = lr
    best_model_name = "Logistic Regression"
    importance = np.abs(best_model.coef_[0])  # å–çµ•å°å€¼ä¾†è¡¨ç¤ºå½±éŸ¿ç¨‹åº¦
else:
    print("âœ… Random Forest è¡¨ç¾æ¯”è¼ƒå¥½ï¼Œå»ºè­°ä½¿ç”¨å®ƒ")
    best_model = rf
    best_model_name = "Random Forest"
    importance = best_model.feature_importances_

# --- ç•«å‡ºæ¢å½¢åœ– ---
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance
})
importance_df['Percentage'] = (importance_df['Importance'] / importance_df['Importance'].sum()) * 100
importance_df = importance_df.sort_values(by="Percentage", ascending=True)

# ç•«åœ–
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Percentage'])
plt.xlabel("Importance (%)")
plt.ylabel("Features")
plt.title(f"Feature Importance - {best_model_name}")
plt.yticks(fontsize=8)
plt.tight_layout()
plt.show()


# --- å»ºç«‹é‡è¦æ€§è³‡æ–™è¡¨ ---
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance
})
importance_df['Percentage'] = (importance_df['Importance'] / importance_df['Importance'].sum()) * 100
importance_df = importance_df.sort_values(by="Percentage", ascending=False).reset_index(drop=True)

# --- åˆ—å°å‡ºæ¯å€‹ç‰¹å¾µä½”çš„æ¯”ä¾‹ ---
print("\nğŸ“Š ç‰¹å¾µé‡è¦æ€§ï¼ˆä¾æ¯”ä¾‹æ’åºï¼‰:")
print(importance_df)

# --- åŒ¯å‡ºç‚º Excel æª”æ¡ˆ ---
excel_filename = f"feature_importance_{best_model_name.replace(' ', '_')}.xlsx"
importance_df.to_excel(excel_filename, index=False)
print(f"\nâœ… ç‰¹å¾µé‡è¦æ€§å·²å„²å­˜ç‚º Excel æª”æ¡ˆï¼š{excel_filename}")
